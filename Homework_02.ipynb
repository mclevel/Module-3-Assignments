{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting otter-grader\n",
      "  Downloading otter_grader-6.1.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting click<9.0.0,>=8.1.7 (from otter-grader)\n",
      "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting dill>=0.3.0 (from otter-grader)\n",
      "  Downloading dill-0.3.9-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting fica>=0.4.1 (from otter-grader)\n",
      "  Downloading fica-0.4.1-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting ipylab<2.0.0,>=1.0.0 (from otter-grader)\n",
      "  Downloading ipylab-1.0.0-py3-none-any.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: ipython in /home/codespace/.local/lib/python3.12/site-packages (from otter-grader) (8.31.0)\n",
      "Collecting ipywidgets<9.0.0,>=8.1.5 (from otter-grader)\n",
      "  Downloading ipywidgets-8.1.5-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: jinja2<4.0,>=3.1 in /home/codespace/.local/lib/python3.12/site-packages (from otter-grader) (3.1.5)\n",
      "Collecting jupytext<2.0.0,>=1.16.4 (from otter-grader)\n",
      "  Downloading jupytext-1.16.6-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: nbconvert>=6.0.0 in /home/codespace/.local/lib/python3.12/site-packages (from nbconvert[webpdf]>=6.0.0; sys_platform != \"emscripten\" and sys_platform != \"wasi\"->otter-grader) (7.16.6)\n",
      "Requirement already satisfied: nbformat>=5.0.0 in /home/codespace/.local/lib/python3.12/site-packages (from otter-grader) (5.10.4)\n",
      "Requirement already satisfied: pandas>=2.0.0 in /home/codespace/.local/lib/python3.12/site-packages (from otter-grader) (2.2.2)\n",
      "Collecting python-on-whales<1.0.0,>=0.72.0 (from otter-grader)\n",
      "  Downloading python_on_whales-0.75.1-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: pyyaml<7,>=6 in /home/codespace/.local/lib/python3.12/site-packages (from otter-grader) (6.0.2)\n",
      "Requirement already satisfied: requests<3.0,>=2.31 in /home/codespace/.local/lib/python3.12/site-packages (from otter-grader) (2.32.3)\n",
      "Collecting wrapt<2.0.0,>=1.16.0 (from otter-grader)\n",
      "  Downloading wrapt-1.17.2-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
      "Collecting docutils (from fica>=0.4.1->otter-grader)\n",
      "  Downloading docutils-0.21.2-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting sphinx (from fica>=0.4.1->otter-grader)\n",
      "  Downloading sphinx-8.1.3-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: comm>=0.1.3 in /home/codespace/.local/lib/python3.12/site-packages (from ipywidgets<9.0.0,>=8.1.5->otter-grader) (0.2.2)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /home/codespace/.local/lib/python3.12/site-packages (from ipywidgets<9.0.0,>=8.1.5->otter-grader) (5.14.3)\n",
      "Collecting widgetsnbextension~=4.0.12 (from ipywidgets<9.0.0,>=8.1.5->otter-grader)\n",
      "  Downloading widgetsnbextension-4.0.13-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting jupyterlab-widgets~=3.0.12 (from ipywidgets<9.0.0,>=8.1.5->otter-grader)\n",
      "  Downloading jupyterlab_widgets-3.0.13-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: decorator in /home/codespace/.local/lib/python3.12/site-packages (from ipython->otter-grader) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/codespace/.local/lib/python3.12/site-packages (from ipython->otter-grader) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in /home/codespace/.local/lib/python3.12/site-packages (from ipython->otter-grader) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/codespace/.local/lib/python3.12/site-packages (from ipython->otter-grader) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /home/codespace/.local/lib/python3.12/site-packages (from ipython->otter-grader) (3.0.50)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /home/codespace/.local/lib/python3.12/site-packages (from ipython->otter-grader) (2.19.1)\n",
      "Requirement already satisfied: stack_data in /home/codespace/.local/lib/python3.12/site-packages (from ipython->otter-grader) (0.6.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/codespace/.local/lib/python3.12/site-packages (from jinja2<4.0,>=3.1->otter-grader) (3.0.2)\n",
      "Collecting markdown-it-py>=1.0 (from jupytext<2.0.0,>=1.16.4->otter-grader)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting mdit-py-plugins (from jupytext<2.0.0,>=1.16.4->otter-grader)\n",
      "  Downloading mdit_py_plugins-0.4.2-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: packaging in /home/codespace/.local/lib/python3.12/site-packages (from jupytext<2.0.0,>=1.16.4->otter-grader) (24.2)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/codespace/.local/lib/python3.12/site-packages (from nbconvert>=6.0.0->nbconvert[webpdf]>=6.0.0; sys_platform != \"emscripten\" and sys_platform != \"wasi\"->otter-grader) (4.12.3)\n",
      "Requirement already satisfied: bleach!=5.0.0 in /home/codespace/.local/lib/python3.12/site-packages (from bleach[css]!=5.0.0->nbconvert>=6.0.0->nbconvert[webpdf]>=6.0.0; sys_platform != \"emscripten\" and sys_platform != \"wasi\"->otter-grader) (6.2.0)\n",
      "Requirement already satisfied: defusedxml in /home/codespace/.local/lib/python3.12/site-packages (from nbconvert>=6.0.0->nbconvert[webpdf]>=6.0.0; sys_platform != \"emscripten\" and sys_platform != \"wasi\"->otter-grader) (0.7.1)\n",
      "Requirement already satisfied: jupyter-core>=4.7 in /home/codespace/.local/lib/python3.12/site-packages (from nbconvert>=6.0.0->nbconvert[webpdf]>=6.0.0; sys_platform != \"emscripten\" and sys_platform != \"wasi\"->otter-grader) (5.7.2)\n",
      "Requirement already satisfied: jupyterlab-pygments in /home/codespace/.local/lib/python3.12/site-packages (from nbconvert>=6.0.0->nbconvert[webpdf]>=6.0.0; sys_platform != \"emscripten\" and sys_platform != \"wasi\"->otter-grader) (0.3.0)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in /home/codespace/.local/lib/python3.12/site-packages (from nbconvert>=6.0.0->nbconvert[webpdf]>=6.0.0; sys_platform != \"emscripten\" and sys_platform != \"wasi\"->otter-grader) (3.1.1)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in /home/codespace/.local/lib/python3.12/site-packages (from nbconvert>=6.0.0->nbconvert[webpdf]>=6.0.0; sys_platform != \"emscripten\" and sys_platform != \"wasi\"->otter-grader) (0.10.2)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /home/codespace/.local/lib/python3.12/site-packages (from nbconvert>=6.0.0->nbconvert[webpdf]>=6.0.0; sys_platform != \"emscripten\" and sys_platform != \"wasi\"->otter-grader) (1.5.1)\n",
      "Collecting playwright (from nbconvert[webpdf]>=6.0.0; sys_platform != \"emscripten\" and sys_platform != \"wasi\"->otter-grader)\n",
      "  Downloading playwright-1.49.1-py3-none-manylinux1_x86_64.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in /home/codespace/.local/lib/python3.12/site-packages (from nbformat>=5.0.0->otter-grader) (2.21.1)\n",
      "Requirement already satisfied: jsonschema>=2.6 in /home/codespace/.local/lib/python3.12/site-packages (from nbformat>=5.0.0->otter-grader) (4.23.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /home/codespace/.local/lib/python3.12/site-packages (from pandas>=2.0.0->otter-grader) (2.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/codespace/.local/lib/python3.12/site-packages (from pandas>=2.0.0->otter-grader) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/codespace/.local/lib/python3.12/site-packages (from pandas>=2.0.0->otter-grader) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/codespace/.local/lib/python3.12/site-packages (from pandas>=2.0.0->otter-grader) (2025.1)\n",
      "Collecting pydantic!=2.0.*,<3,>=2 (from python-on-whales<1.0.0,>=0.72.0->otter-grader)\n",
      "  Downloading pydantic-2.10.6-py3-none-any.whl.metadata (30 kB)\n",
      "Requirement already satisfied: typing-extensions in /home/codespace/.local/lib/python3.12/site-packages (from python-on-whales<1.0.0,>=0.72.0->otter-grader) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3.0,>=2.31->otter-grader) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3.0,>=2.31->otter-grader) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3.0,>=2.31->otter-grader) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3.0,>=2.31->otter-grader) (2024.8.30)\n",
      "Requirement already satisfied: webencodings in /home/codespace/.local/lib/python3.12/site-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert>=6.0.0->nbconvert[webpdf]>=6.0.0; sys_platform != \"emscripten\" and sys_platform != \"wasi\"->otter-grader) (0.5.1)\n",
      "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /home/codespace/.local/lib/python3.12/site-packages (from bleach[css]!=5.0.0->nbconvert>=6.0.0->nbconvert[webpdf]>=6.0.0; sys_platform != \"emscripten\" and sys_platform != \"wasi\"->otter-grader) (1.4.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /home/codespace/.local/lib/python3.12/site-packages (from jedi>=0.16->ipython->otter-grader) (0.8.4)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /home/codespace/.local/lib/python3.12/site-packages (from jsonschema>=2.6->nbformat>=5.0.0->otter-grader) (25.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/codespace/.local/lib/python3.12/site-packages (from jsonschema>=2.6->nbformat>=5.0.0->otter-grader) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/codespace/.local/lib/python3.12/site-packages (from jsonschema>=2.6->nbformat>=5.0.0->otter-grader) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/codespace/.local/lib/python3.12/site-packages (from jsonschema>=2.6->nbformat>=5.0.0->otter-grader) (0.22.3)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /home/codespace/.local/lib/python3.12/site-packages (from jupyter-core>=4.7->nbconvert>=6.0.0->nbconvert[webpdf]>=6.0.0; sys_platform != \"emscripten\" and sys_platform != \"wasi\"->otter-grader) (4.3.6)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=1.0->jupytext<2.0.0,>=1.16.4->otter-grader)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /home/codespace/.local/lib/python3.12/site-packages (from nbclient>=0.5.0->nbconvert>=6.0.0->nbconvert[webpdf]>=6.0.0; sys_platform != \"emscripten\" and sys_platform != \"wasi\"->otter-grader) (8.6.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/codespace/.local/lib/python3.12/site-packages (from pexpect>4.3->ipython->otter-grader) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /home/codespace/.local/lib/python3.12/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython->otter-grader) (0.2.13)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic!=2.0.*,<3,>=2->python-on-whales<1.0.0,>=0.72.0->otter-grader)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.27.2 (from pydantic!=2.0.*,<3,>=2->python-on-whales<1.0.0,>=0.72.0->otter-grader)\n",
      "  Downloading pydantic_core-2.27.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in /home/codespace/.local/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->otter-grader) (1.17.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/codespace/.local/lib/python3.12/site-packages (from beautifulsoup4->nbconvert>=6.0.0->nbconvert[webpdf]>=6.0.0; sys_platform != \"emscripten\" and sys_platform != \"wasi\"->otter-grader) (2.6)\n",
      "Collecting greenlet==3.1.1 (from playwright->nbconvert[webpdf]>=6.0.0; sys_platform != \"emscripten\" and sys_platform != \"wasi\"->otter-grader)\n",
      "  Downloading greenlet-3.1.1-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting pyee==12.0.0 (from playwright->nbconvert[webpdf]>=6.0.0; sys_platform != \"emscripten\" and sys_platform != \"wasi\"->otter-grader)\n",
      "  Downloading pyee-12.0.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting sphinxcontrib-applehelp>=1.0.7 (from sphinx->fica>=0.4.1->otter-grader)\n",
      "  Downloading sphinxcontrib_applehelp-2.0.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting sphinxcontrib-devhelp>=1.0.6 (from sphinx->fica>=0.4.1->otter-grader)\n",
      "  Downloading sphinxcontrib_devhelp-2.0.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting sphinxcontrib-htmlhelp>=2.0.6 (from sphinx->fica>=0.4.1->otter-grader)\n",
      "  Downloading sphinxcontrib_htmlhelp-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting sphinxcontrib-jsmath>=1.0.1 (from sphinx->fica>=0.4.1->otter-grader)\n",
      "  Downloading sphinxcontrib_jsmath-1.0.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting sphinxcontrib-qthelp>=1.0.6 (from sphinx->fica>=0.4.1->otter-grader)\n",
      "  Downloading sphinxcontrib_qthelp-2.0.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting sphinxcontrib-serializinghtml>=1.1.9 (from sphinx->fica>=0.4.1->otter-grader)\n",
      "  Downloading sphinxcontrib_serializinghtml-2.0.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting snowballstemmer>=2.2 (from sphinx->fica>=0.4.1->otter-grader)\n",
      "  Downloading snowballstemmer-2.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: babel>=2.13 in /home/codespace/.local/lib/python3.12/site-packages (from sphinx->fica>=0.4.1->otter-grader) (2.16.0)\n",
      "Collecting alabaster>=0.7.14 (from sphinx->fica>=0.4.1->otter-grader)\n",
      "  Downloading alabaster-1.0.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting imagesize>=1.3 (from sphinx->fica>=0.4.1->otter-grader)\n",
      "  Downloading imagesize-1.4.1-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: executing>=1.2.0 in /home/codespace/.local/lib/python3.12/site-packages (from stack_data->ipython->otter-grader) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /home/codespace/.local/lib/python3.12/site-packages (from stack_data->ipython->otter-grader) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in /home/codespace/.local/lib/python3.12/site-packages (from stack_data->ipython->otter-grader) (0.2.3)\n",
      "Requirement already satisfied: pyzmq>=23.0 in /home/codespace/.local/lib/python3.12/site-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert>=6.0.0->nbconvert[webpdf]>=6.0.0; sys_platform != \"emscripten\" and sys_platform != \"wasi\"->otter-grader) (26.2.0)\n",
      "Requirement already satisfied: tornado>=6.2 in /home/codespace/.local/lib/python3.12/site-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert>=6.0.0->nbconvert[webpdf]>=6.0.0; sys_platform != \"emscripten\" and sys_platform != \"wasi\"->otter-grader) (6.4.2)\n",
      "Downloading otter_grader-6.1.0-py3-none-any.whl (142 kB)\n",
      "Downloading click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Downloading dill-0.3.9-py3-none-any.whl (119 kB)\n",
      "Downloading fica-0.4.1-py3-none-any.whl (13 kB)\n",
      "Downloading ipylab-1.0.0-py3-none-any.whl (100 kB)\n",
      "Downloading ipywidgets-8.1.5-py3-none-any.whl (139 kB)\n",
      "Downloading jupytext-1.16.6-py3-none-any.whl (154 kB)\n",
      "Downloading python_on_whales-0.75.1-py3-none-any.whl (114 kB)\n",
      "Downloading wrapt-1.17.2-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (89 kB)\n",
      "Downloading jupyterlab_widgets-3.0.13-py3-none-any.whl (214 kB)\n",
      "Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
      "Downloading pydantic_core-2.27.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading widgetsnbextension-4.0.13-py3-none-any.whl (2.3 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading docutils-0.21.2-py3-none-any.whl (587 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m587.4/587.4 kB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mdit_py_plugins-0.4.2-py3-none-any.whl (55 kB)\n",
      "Downloading playwright-1.49.1-py3-none-manylinux1_x86_64.whl (44.2 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m44.2/44.2 MB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading greenlet-3.1.1-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (613 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m613.1/613.1 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyee-12.0.0-py3-none-any.whl (14 kB)\n",
      "Downloading sphinx-8.1.3-py3-none-any.whl (3.5 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m50.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading alabaster-1.0.0-py3-none-any.whl (13 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading imagesize-1.4.1-py2.py3-none-any.whl (8.8 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Downloading snowballstemmer-2.2.0-py2.py3-none-any.whl (93 kB)\n",
      "Downloading sphinxcontrib_applehelp-2.0.0-py3-none-any.whl (119 kB)\n",
      "Downloading sphinxcontrib_devhelp-2.0.0-py3-none-any.whl (82 kB)\n",
      "Downloading sphinxcontrib_htmlhelp-2.1.0-py3-none-any.whl (98 kB)\n",
      "Downloading sphinxcontrib_jsmath-1.0.1-py2.py3-none-any.whl (5.1 kB)\n",
      "Downloading sphinxcontrib_qthelp-2.0.0-py3-none-any.whl (88 kB)\n",
      "Downloading sphinxcontrib_serializinghtml-2.0.0-py3-none-any.whl (92 kB)\n",
      "Installing collected packages: snowballstemmer, wrapt, widgetsnbextension, sphinxcontrib-serializinghtml, sphinxcontrib-qthelp, sphinxcontrib-jsmath, sphinxcontrib-htmlhelp, sphinxcontrib-devhelp, sphinxcontrib-applehelp, pyee, pydantic-core, mdurl, jupyterlab-widgets, imagesize, greenlet, docutils, dill, click, annotated-types, alabaster, sphinx, pydantic, playwright, markdown-it-py, python-on-whales, mdit-py-plugins, ipywidgets, fica, ipylab, jupytext, otter-grader\n",
      "Successfully installed alabaster-1.0.0 annotated-types-0.7.0 click-8.1.8 dill-0.3.9 docutils-0.21.2 fica-0.4.1 greenlet-3.1.1 imagesize-1.4.1 ipylab-1.0.0 ipywidgets-8.1.5 jupyterlab-widgets-3.0.13 jupytext-1.16.6 markdown-it-py-3.0.0 mdit-py-plugins-0.4.2 mdurl-0.1.2 otter-grader-6.1.0 playwright-1.49.1 pydantic-2.10.6 pydantic-core-2.27.2 pyee-12.0.0 python-on-whales-0.75.1 snowballstemmer-2.2.0 sphinx-8.1.3 sphinxcontrib-applehelp-2.0.0 sphinxcontrib-devhelp-2.0.0 sphinxcontrib-htmlhelp-2.1.0 sphinxcontrib-jsmath-1.0.1 sphinxcontrib-qthelp-2.0.0 sphinxcontrib-serializinghtml-2.0.0 widgetsnbextension-4.0.13 wrapt-1.17.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install otter-grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"Homework_02.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 02:  Linear Regression in Theory and Practice\n",
    "\n",
    "In this homework, you will learn how to create and explore a simple, synthetic regression dataset using scikit‚Äêlearn‚Äôs `make_regression` function. We often use synthetic data to test out our modeling workflows, gain practical experience with various models, and observe how noise, training set size, and other factors affect model performance, as measured by the Mean Squared Error (MSE). By controlling the dataset generation, we can see precisely how well the model approximates the dataset parameters, and you‚Äôll practice splitting data into training and testing sets to estimate how well the model will generalize to unseen data. This will give you insight into how regression models behave under different conditions (such as varying amounts of training data) and help you interpret outcomes with appropriate metrics. \n",
    "\n",
    "In some problems you will need to do a bit of research in `sklearn`'s documentation. The functions we are using are very common in ML, and it will be well worth the time you spend reading through the documentation and looking at the examples provided there. \n",
    "\n",
    "As I mentioned in the first Live Session, some things I will ask you do to are not graded; I expect you to do them just as thoroughly as the answers which will be graded.  To skip the non-graded parts\n",
    "and just focus on the graded portions will waste your time and money, and result in a less-than-expected return on both when you work on projects and proceed to later modules, and, eventually, to a career in data science. \n",
    "\n",
    "#### One more things before you start:  \n",
    "\n",
    "This is the last homework in which you will get instant feedback on all your solutions; since Gradescope is not set up for this, we have used the Otter auto-grader to allow immediate feedback in this notebook. After each cell in which you assign a value to a variable such as a1a etc. there is a cell which will check your solution. Therefore, you may test your solutions as much as you wish before submitting to Gradescope. Starting with HW 03, you will continue to have graded and ungraded problems, but you will not receive feedback on the graded parts until after the late submission period is over. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful imports and utilities\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import kagglehub\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import fetch_california_housing,make_regression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from math import isclose\n",
    "\n",
    "# globals\n",
    "\n",
    "random_state = 42\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem One:  Generate and Display a Simple Univariant Regression Dataset\n",
    "\n",
    "In this first problem, we are going to experiment with `sklearn`'s `make_regression` function, which can generate synthetic datasets with various characteristics, including the number of samples, the standard deviation of the errors, and many, many others.   Oddly, it makes you choose the bias, but then it randomly chooses the other coefficients; it also does not allow you to specify the range of the features -- however, it would be easy to write your own version which does these things.\n",
    "\n",
    "\n",
    "Using `make_regression` is a commonly used to test out frameworks and investigate the properties of models; we'll use it again when we investigate feature selection strategies in Week 5.\n",
    "\n",
    "\n",
    "Before you start, read through the `sklearn` documentation on `make_regression`. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A\n",
    "\n",
    "Generate a univariate dataset $X, y$ using `make_regression` with\n",
    "   - 20 samples\n",
    "   - error standard deviation of 20 (called `noise` in the function)\n",
    "   - y-intercept of 0.5 (called `bias` in the function)\n",
    "   - `random_state = 42`\n",
    "   - `coef = True`  (this will return the coefficients of the underlying model)\n",
    "   \n",
    "This will return a tuple with 3 values (read the docs!). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Your code here \n",
    "X, y, coef = make_regression(n_samples=20, n_features=1, bias = 0.5, noise=20, random_state=42, coef = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B\n",
    "\n",
    "- Set `a1b` to the range of `X`, i.e., a list `[lb,ub]` (or tuple or array) where `lb` is the smallest value in `X` and `ub` is the largest, both rounded to 4 decimal places.\n",
    "- Note: You can use `np.around(...)` for just about anything, including floats and tuples.  For numpy arrays, generally it is more readable to use the postfix version  `.round(...)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range of X = (np.float64(-1.9133), np.float64(1.5792))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Your code here\n",
    "\n",
    "a1b = (np.min(X[:,0]).round(4), np.max(X[:,0]).round(4))\n",
    "\n",
    "print(f'Range of X = {a1b}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>q1b</pre></strong> passed! üíØ</p>"
      ],
      "text/plain": [
       "q1b results: All test cases passed!"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q1b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part C\n",
    "\n",
    "- Set `a1c` to the slope of the underlying model, to 4 decimal places. \n",
    "\n",
    "- Hint: `make_regression` will return the coefficients as an array with one fewer dimensions than `X`; in this case, it is a 0-dimension array whose shape is `()`.  Weird, I know, but this is unusual.  You can pretend it is just a float. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slope = 45.606998421703594\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Your code here\n",
    "\n",
    "a1c = coef\n",
    "\n",
    "print(f'Slope = {a1c}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>q1c</pre></strong> passed! üöÄ</p>"
      ],
      "text/plain": [
       "q1c results: All test cases passed!"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q1c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part D\n",
    "\n",
    "Generate a scatterplot of the dataset with the following:\n",
    "   - A suitable title\n",
    "   - Figure size of (8,6)\n",
    "   - Suitable labels on x and y axis (just `X` and y are fine)\n",
    "   - The underlying model (with no noise), a line extending through the range of X, using `color=grey`\n",
    "   - A label \"Underlying Model\" on the line representing the model and a label \"Data Points\" on\n",
    "     the sample points. (Don't forget to call `plt.legend()` to show the labels!)\n",
    "\n",
    "Note: Optional, but playing around with the style of the scatter points will produce a better-looking plot; I use `marker='.'` to get smaller dots for a scatterplot; you can\n",
    "also play around with `linestyle='--'` or other choices, plus of course colors, saturation `alpha=0.5`, etc. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Two: Run Linear Regression on the Data Set and Evaluate the Results\n",
    "\n",
    "Now we will use `sklearn`'s `LinearRegression` model to create a model from the dataset. Of course, the **underlying model** has already been\n",
    "created, but your linear regression won't know that, and it has to determine the best model given the data samples it has.\n",
    "\n",
    "### Part A: Create and Evaluate a Linear Model\n",
    "- Create a linear regression model and train it on `X,y`. \n",
    "- Set `a2a` to the bias/y-intercept of the model, to 4 decimal places.            \n",
    "- NOTE:  You must round the value assigned to `a2a`, NOT just print it out with 4 digits of precision.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Your code here\n",
    "\n",
    "a2a = ...\n",
    "\n",
    "print(f'Bias = {a2a}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B\n",
    "\n",
    "- Set `a2b` to the slope of the model.\n",
    "- Hint: The coefficients are returned as a 1-dimensional array (unlike make_regression!), so you'll need to turn a 1-element array into a scalar. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "a2b = ...\n",
    "\n",
    "print(f'Slope = {a2b}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part C\n",
    "\n",
    "- Set `a2c` to the **training MSE** of the model on the dataset, to 4 decimal places\n",
    "- Hint: generate an array `y_pred` by using the model to predict the targets from the original `X`, then calculate the mean squared error using the appropriate `sklearn` function.  (Now might be a good time to look at that first code cell to see what I imported for you.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Your code here\n",
    "\n",
    "a2c = ...\n",
    "\n",
    "print(f'Training MSE = {a2c}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part D\n",
    "\n",
    "- Set `a2d` to the coefficient of determination (R2) of the model (read the docs!), to 4 decimal places\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Your code here\n",
    "\n",
    "a2d = ...\n",
    "\n",
    "print(f'Training MSE = {a2d}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part E\n",
    "\n",
    "- Provide a visualization of the regression line by cutting and pasting the code from Problem One D, then adding a plot of the model's regression line in red. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pause and Ponder (no need to write answers, just think about these):  \n",
    "- Why does the linear regression line not match the underlying model?\n",
    "- Which parameters (`bias`, `n_samples`,`noise`) do you think affect how well the regression model matches the actual model?\n",
    "- What changes to these parameters would result in a more accurate match between underlying and regression models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Three:  How well does it generalize?\n",
    "\n",
    "The **most important issue** in making useful models is to ensure that they are able to **generalize to new data from the same domain.**  For example, if you create a model from a housing price dataset, \n",
    "you want it to be able to predict what price could be obtained if you build new houses with particular features. You will learn techniques for judging how well models generalize in\n",
    "the next few lessons, and it will continue to be a crucial issue going forward. \n",
    "\n",
    "For now, since we have the underlying model (which never happens IRL!) we can easily create new data samples with the same characteristics as the set we used for training. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A\n",
    "\n",
    "- Complete the following stub to create a function to create new points to add the data set, with all the same parameters as the underlying model, following these steps:\n",
    "    1. Create a random number within the existing range of`X`, using `np.random.uniform` \n",
    "    2. Use the bias and slope of the underlying model to find the point (x,y) on the regression line (which is the prediction for y given x)\n",
    "    3. Return (x,y)\n",
    "\n",
    "- Test it by running the cell repeatedly to see the results (we are not setting a random seed, so it will generate random answers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def generate_sample():\n",
    "    # Your code here\n",
    "    return ...\n",
    "\n",
    "generate_sample()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B\n",
    "\n",
    "- Generate 5 new data points (we'll use these below as a **test set**) and assign them to ndarrays `X_new` and `y_new`, \n",
    "- Hint: create a list of pairs and split using `zip(* ...)`\n",
    "\n",
    "- Set `a3b` to the first 2 values in `X_new`, rounded to 4 decimal places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "np.random.seed(42)             # do not remove this line!\n",
    "\n",
    "# Your code here\n",
    "\n",
    "a3b = ...\n",
    "\n",
    "print(a3b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part C\n",
    "\n",
    "- Cut and paste the code from Problem Two E and add one line of code to display the new data points in green, in addition to what you displayed in Problem Two. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part D\n",
    "\n",
    "- Assign the MSE on the new data to `a3d` (we'll later call this the \"test MSE\") , rounded to 4 decimal points.\n",
    "- Hint: When you \"roll your own\" datasets using ndarrays, you will generally have to reshape them using `.reshape(-1,1)` because `sklearn` models expect a column array, not a \"normal\" array. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "a3d = ...\n",
    "\n",
    "print(f'MSE on new data = {a3d}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part E:  Let's Compare Metrics\n",
    "\n",
    "We now have three related values:\n",
    "- Noise = standard deviation of \"errors\" between the data and the underlying model \n",
    "- Training MSE of the linear model on the dataset \n",
    "- Testing MSE of the linear model on new data generated with the same parameters as the original dataset\n",
    "\n",
    "Answer the following multiple-choice problems by assigning the variable to the  **most accurate** statement.\n",
    "\n",
    "#### E1) Comparing the Two MSEs\n",
    "\n",
    "Why might the training MSE be *larger* than the testing MSE in this scenario?\n",
    "\n",
    "1. These should be exactly the same, so there must have been an error somewhere.  \n",
    "2.  With only 20 training points, a few unusual data points (outliers) can increase the average training error; meanwhile, the small test set of 5 points could *by chance* lead to smaller errors overall.  \n",
    "3. There is always a positive bias in the regression line, so it will always be larger. \n",
    "4.  There is very little relationship between these two numbers, so the fact that they are close to each other must be an accident.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "a3e1 =  ...            # your answer should one of 1, 2, 3, 4 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3e1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### E2) Understanding the Noise Parameter\n",
    " \n",
    "Suppose a dataset has been generated using `make_regression` with `noise = 20`.  \n",
    "\n",
    "Which statement best explains the significance of the noise parameter when interpreting the MSE of models trained on this dataset?  \n",
    "\n",
    "1. The square of the noise parameter, $20^2 = 400$, represents the irreducible noise in the data; no model can achieve an MSE lower than this on average.  \n",
    "2. A noise standard deviation of 20 means the MSE can eventually be reduced to 0 with enough data.  \n",
    "3. MSE measures the *average* absolute errors, so having $\\sigma = 20$ implies the MSE will always equal 20.  \n",
    "4. If the noise is 20, then it‚Äôs possible to create a model with training and testing MSEs that are *exactly* 400 with sufficient effort.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "a3e2 =  ...            # your answer should one of 1, 2, 3, 4 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3e2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### E3) The Role of Dataset Size\n",
    "\n",
    "You fitted a linear model on **20** training points and tested it on **5** new points, both drawn from a process with noise standard deviation 20 (variance 400).  \n",
    "\n",
    "What would happen if we repeated the same experiment with more training/testing points or with even fewer points?  \n",
    "In other words, **how does dataset size affect the measured MSE values?**\n",
    "\n",
    "1.  If the training set is small, the MSE will always be *exactly* 400 for both training and test sets, since there‚Äôs too little data to deviate from the noise variance.  \n",
    "\n",
    "2.  Collecting more data actively *lowers* the true noise standard deviation from 20 to something smaller, guaranteeing an MSE below 400.  \n",
    "3. If you have fewer than 30 data points, the training MSE must always exceed 400 and the test MSE must always be *less* than 400.  \n",
    "4. Small sample sizes can cause large swings in MSE, sometimes pushing the training MSE above 400 while letting a tiny test set fall below 400 by chance. With larger datasets, the MSE typically stabilizes closer to 400.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "a3e3 =  ...            # your answer should one of 1, 2, 3, 4  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3e3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Four: Linear Regression on an Actual Dataset (finally!)\n",
    "\n",
    "Let's consider applying what we have learned to an actual dataset, the Diabetes dataset from Kaggle. This has 10 features and 1 target,\n",
    "so it is an instance of **multiple regression**, however we can train a model almost exactly as we did in the univariate case. \n",
    "\n",
    "After doing a bit of EDA and massaging of the features, we will first consider separate regressions on a single feature of the dataset, and then run multiple regression on the whole set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Kaggle Diabetes Dataset\n",
    "\n",
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "data_diabetes = load_diabetes(as_frame=True)\n",
    "df_diabetes = pd.concat([data_diabetes.data, data_diabetes.target.rename('DiseaseProgression')], axis=1)\n",
    "\n",
    "feature_names = df_diabetes.drop(columns=[\"DiseaseProgression\"]).columns.tolist()\n",
    "\n",
    "df_diabetes.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_diabetes.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features of the Diabetes Dataset\n",
    "\n",
    "- **age**: age of the patient  \n",
    "- **sex**: gender of the patient  \n",
    "- **bmi**: body mass index (BMI)  \n",
    "- **bp**:  mean blood pressure  \n",
    "- **s1**:  measure of serum cholesterol levels  \n",
    "- **s2**:  measure related to low-density lipoproteins (LDL)  \n",
    "- **s3**:  measure of high-density lipoproteins (HDL)  \n",
    "- **s4**:  measure of total cholesterol-to-HDL ratio  \n",
    "- **s5**:  measure of serum triglycerides  \n",
    "- **s6**:  measure of blood sugar levels  \n",
    "- **DiseaseProgression**: Quantitative measure of diabetes disease progression one year after baseline (target variable)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This one we'll give you:\n",
    "# Always useful to create histograms of the features when possible; the layout and formatting are sometimes awkward, so\n",
    "# I use the following:\n",
    "\n",
    "df_diabetes.hist(figsize=(12,8), layout=(3,4),grid=False,edgecolor='black')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A:\n",
    "\n",
    "- Generate the correlation matrix for all the features using Pandas' `.corr()` function, plot it as a heatmap, and give it an appropriate title.   \n",
    "- Hint: Use `seaborn`'s `heatmap` function, imported in the first cell as `sns`.  (Have I mentioned you might want to read the docs?)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B\n",
    "\n",
    "- Now set `a4b` to the name of the feature which has the largest correlation with the target. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "a4b =  ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part C\n",
    "\n",
    "- Next, in order to avoid any snickers when discussing the dataset, change the column name 'sex' to 'gender'\n",
    "  in place and set the variable `a4c` to a numpy array of the feature/column names.\n",
    "- Hint: if your answer starts `Index(...` then you have a Pandas data structure and not an ndarray as required. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "a4c = ...\n",
    "\n",
    "print(f'Feature names = {a4c}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part D\n",
    "\n",
    "- Create dataset in the form `X,y` from the dataframe by dropping the last column to create `X`, and just selecting the last column to make `y`\n",
    "\n",
    "- Note: `sklearn`'s models are perfectly happy to work with dataframes, so we can just keep them as such and not convert to ndarrays.  Two advantages are: you don't have to reshape for input to the model, and you keep the feature names in case you need them later, e.g., when doing feature selection. \n",
    "\n",
    "- Set `aa4d` to the pair  ( shape of `X`, shape of `y` )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "a4d = ...\n",
    "\n",
    "print(f'(shape of X, shape of y) = {a4d}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part E\n",
    "\n",
    "\n",
    "Now we will create two regression plots, the first for a univariate regression of just the most highly correlated feature, and\n",
    "then on the whole data set.  (We'll return to the issue of feature importance in later weeks.)\n",
    "\n",
    "- Create `X_uni` from `X` by selecting only the most highly correlated feature, and run a univariate regression with it against the target. \n",
    "- Assign `a4e` the **training MSE** by using the model to predict on `X_uni` and comparing the values with `y`. \n",
    "- As usual, round all floats to 4 decimal places.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "a4e = ...\n",
    "\n",
    "print(f'MSE on most correlated feature = {a4e}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4e\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part F\n",
    "\n",
    "- Now run multiple regression on the entire `X` and set `a4f` to the **training MSE** on `X`, rounded to 4 decimal places.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "a4f = ...\n",
    "\n",
    "print(f'MSE on whole set = {a4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part G: Testing for Generalization\n",
    "\n",
    "We will spend the next two lessons thinking about how to evaluate  models for generalization, but let's try a naive strategy\n",
    "for now:  We will split the dataset into training and testing sets, and see how the model performs on  data it has never seen.\n",
    "The disadvantage of this is that we have less training data, of course!\n",
    "\n",
    "- Use `sklearn`'s `train_test_split` to shuffle `X` and split it into 80% training data and 20% testing data with `random_state=42`\n",
    "- Train a model `model_training` on the training set, and then test it on the same set to find the training MSE. \n",
    "- Assign the training MSE to `a4g`, with... you guessed it... 4 decimal points.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "a4g = ...\n",
    "\n",
    "print(f'Training MSE = {a4g}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4g\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part H\n",
    "\n",
    "- Run the model created in Part G on the testing set to determine the test MSE. \n",
    "- Set `a4h` to the test MSE, to 4 decimal points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "a4h = ...\n",
    "\n",
    "print(f'MSE on most correlated feature = {a4h}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4h\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part I\n",
    "\n",
    "Now try setting the percentage of the test size to different values, perhaps 0.1, 0.2, 0.3, 0.4, and 0.5 and run the above\n",
    "cells and observe the training and testing MSEs. Then choose the best answer below. \n",
    "\n",
    "\n",
    "**How Does the Training‚ÄêSet Size Affect MSE?**\n",
    "\n",
    "1.  Both **training MSE** and **testing MSE** remain exactly the same regardless of how many points you use, provided you keep the `random_state` fixed.  \n",
    "2.  Both **training MSE** and **testing MSE** steadily converge to **zero** once you exceed a certain training‚Äêset size threshold (e.g., 30 points).  \n",
    "3.  **Training MSE** usually goes **up** slightly with a bigger training set (it‚Äôs harder to fit more points perfectly), but **testing MSE** tends to go **down** (the model generalizes better with more data).  \n",
    "4.  Both **training MSE** and **testing MSE** decrease when the training set grows, because the model memorizes a larger volume of data and thus reduces all errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "a4i = ...            # your answer should one of 1, 2, 3, 4 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4i\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part J\n",
    "\n",
    "How many hours did you spend completing this homework? Set `a4j` to a integer giving the answer to this question. Any integer will receive full credit in the autograder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "a4j = ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4j\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {
    "q1b": {
     "name": "q1b",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> # BEGIN TEST CONFIG\n>>> points: 2.5\n>>> hidden: True\n>>> # END TEST CONFIG\n>>> # HIDDEN\n>>> assert isclose(a1b[0],-1.9133, abs_tol=0.02) and isclose(a1b[1],1.5792, abs_tol=0.02)\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1c": {
     "name": "q1c",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> # BEGIN TEST CONFIG\n>>> points: 2.5\n>>> hidden: True\n>>> # END TEST CONFIG\n>>> # HIDDEN\n>>> assert isclose(a1c,45.607, abs_tol=0.02)\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2a": {
     "name": "q2a",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> # BEGIN TEST CONFIG\n>>> points: 2.5\n>>> hidden: True\n>>> # END TEST CONFIG\n>>> # HIDDEN\n>>> assert isclose(a2a,5.2857, abs_tol=0.02)\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2b": {
     "name": "q2b",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> # BEGIN TEST CONFIG\n>>> points: 2.5\n>>> hidden: True\n>>> # END TEST CONFIG\n>>> # HIDDEN\n>>> assert isclose(a2b,45.7125, abs_tol=0.02)\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2c": {
     "name": "q2c",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> # BEGIN TEST CONFIG\n>>> points: 2.5\n>>> hidden: True\n>>> # END TEST CONFIG\n>>> # HIDDEN\n>>> assert isclose(a2c,436.913, abs_tol=0.02)\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2d": {
     "name": "q2d",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> # BEGIN TEST CONFIG\n>>> points: 2.5\n>>> hidden: True\n>>> # END TEST CONFIG\n>>> # HIDDEN\n>>> assert isclose(a2d,0.8072, abs_tol=0.02)\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3b": {
     "name": "q3b",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> # BEGIN TEST CONFIG\n>>> points: 2.5\n>>> hidden: True\n>>> # END TEST CONFIG\n>>> # HIDDEN\n>>> assert isclose(a3b[0],-0.6052, abs_tol=0.02) and isclose(a3b[1],-1.3685, abs_tol=0.02)\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3d": {
     "name": "q3d",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> # BEGIN TEST CONFIG\n>>> points: 2.5\n>>> hidden: True\n>>> # END TEST CONFIG\n>>> # HIDDEN\n>>> assert isclose(a3d,245.9093, abs_tol=0.02)\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3e1": {
     "name": "q3e1",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> # BEGIN TEST CONFIG\n>>> points: 2.5\n>>> hidden: True\n>>> # END TEST CONFIG\n>>> # HIDDEN\n>>> assert a3e1 == 2\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3e2": {
     "name": "q3e2",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> # BEGIN TEST CONFIG\n>>> points: 2.5\n>>> hidden: True\n>>> # END TEST CONFIG\n>>> # HIDDEN\n>>> assert a3e2 == 1\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3e3": {
     "name": "q3e3",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> # BEGIN TEST CONFIG\n>>> points: 2.5\n>>> hidden: True\n>>> # END TEST CONFIG\n>>> # HIDDEN\n>>> assert a3e3 == 4\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4b": {
     "name": "q4b",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> # BEGIN TEST CONFIG\n>>> points: 2.5\n>>> hidden: True\n>>> # END TEST CONFIG\n>>> # HIDDEN\n>>> assert a4b == 'bmi'\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4c": {
     "name": "q4c",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> # BEGIN TEST CONFIG\n>>> points: 2.5\n>>> hidden: True\n>>> # END TEST CONFIG\n>>> # HIDDEN\n>>> assert len(a4c) == 11 and (a4c == np.array(['age','gender','bmi','bp','s1','s2','s3','s4','s5','s6','DiseaseProgression'])).all()\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4d": {
     "name": "q4d",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> # BEGIN TEST CONFIG\n>>> points: 2.5\n>>> hidden: True\n>>> # END TEST CONFIG\n>>> # HIDDEN\n>>> assert a4d[0] == (442,10) and a4d[1][0] == 442\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4e": {
     "name": "q4e",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> # BEGIN TEST CONFIG\n>>> points: 2.5\n>>> hidden: True\n>>> # END TEST CONFIG\n>>> # HIDDEN\n>>> assert isclose(a4e,3890.4566,abs_tol=0.02)\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4f": {
     "name": "q4f",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> # BEGIN TEST CONFIG\n>>> points: 2.5\n>>> hidden: True\n>>> # END TEST CONFIG\n>>> # HIDDEN\n>>> assert isclose(a4f,2859.6963,abs_tol=0.02)\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4g": {
     "name": "q4g",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> # BEGIN TEST CONFIG\n>>> points: 2.5\n>>> hidden: True\n>>> # END TEST CONFIG\n>>> # HIDDEN\n>>> assert isclose(a4g,2868.5497,abs_tol=0.02)\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4h": {
     "name": "q4h",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> # BEGIN TEST CONFIG\n>>> points: 2.5\n>>> hidden: True\n>>> # END TEST CONFIG\n>>> # HIDDEN\n>>> assert isclose(a4h,2900.1936,abs_tol=0.02)\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4i": {
     "name": "q4i",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> # BEGIN TEST CONFIG\n>>> points: 2.5\n>>> hidden: True\n>>> # END TEST CONFIG\n>>> # HIDDEN\n>>> assert a4i == 3\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4j": {
     "name": "q4j",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> # BEGIN TEST CONFIG\n>>> points: 2.5 \n>>> hidden: True\n>>> # END TEST CONFIG\n>>> # HIDDEN\n>>> assert True\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
